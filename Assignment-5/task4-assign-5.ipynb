{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87862df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE / POTENTIALLY BIASED (example)\n",
    "def score_applicant_naive(applicant: dict) -> float:\n",
    "    \"\"\"\n",
    "    Naive scoring that (dangerously) uses gender as a direct multiplier.\n",
    "    applicant keys: education_years, years_experience, skill_score (0-100),\n",
    "                    certifications_count, gender ('male'|'female'|'other')\n",
    "    \"\"\"\n",
    "    base = 0.4 * applicant['education_years'] + \\\n",
    "           0.4 * applicant['years_experience'] + \\\n",
    "           0.002 * applicant['skill_score'] + \\\n",
    "           0.5 * applicant['certifications_count']\n",
    "    # RISK: injecting gender factor directly can cause discrimination\n",
    "    gender = applicant.get('gender', '').lower()\n",
    "    if gender == 'male':\n",
    "        base *= 1.05   # unfair boost for males\n",
    "    elif gender == 'female':\n",
    "        base *= 0.95   # unfair penalty for females\n",
    "    # return a numeric score\n",
    "    return round(base, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "820f0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAFER: scoring without gender and with normalization\n",
    "from typing import Dict\n",
    "import math\n",
    "\n",
    "def score_applicant_safe(applicant: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Safer applicant scoring that does NOT use gender or other protected attributes.\n",
    "    Expected applicant keys: education_years, years_experience, skill_score (0-100),\n",
    "    certifications_count. Missing keys default to 0.\n",
    "\n",
    "    Returns:\n",
    "        float: normalized score in range [0, 100]\n",
    "    \"\"\"\n",
    "    # extract features with safe defaults\n",
    "    edu = float(applicant.get('education_years', 0))\n",
    "    exp = float(applicant.get('years_experience', 0))\n",
    "    skill = float(applicant.get('skill_score', 0))  # 0-100\n",
    "    certs = float(applicant.get('certifications_count', 0))\n",
    "\n",
    "    # feature scaling (simple): map features to roughly comparable ranges\n",
    "    # education_years: assume 0-20 -> normalized to 0-1\n",
    "    edu_norm = min(edu / 20.0, 1.0)\n",
    "    # experience: 0-30 years -> 0-1\n",
    "    exp_norm = min(exp / 30.0, 1.0)\n",
    "    # skill: already 0-100 -> 0-1\n",
    "    skill_norm = min(max(skill, 0.0), 100.0) / 100.0\n",
    "    # certifications: treat 0-10 -> 0-1\n",
    "    cert_norm = min(certs / 10.0, 1.0)\n",
    "\n",
    "    # weighted sum (weights sum to 1)\n",
    "    w_edu, w_exp, w_skill, w_cert = 0.2, 0.3, 0.4, 0.1\n",
    "    raw = w_edu * edu_norm + w_exp * exp_norm + w_skill * skill_norm + w_cert * cert_norm\n",
    "\n",
    "    # map to 0-100 score\n",
    "    score = raw * 100.0\n",
    "    # optional: apply a small smoothing\n",
    "    return round(score, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bdd654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_checks.py\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "def group_stats(applicants: List[Dict], score_fn: Callable, threshold: float = 50.0, group_key: str = 'gender'):\n",
    "    \"\"\"\n",
    "    Compute average score and acceptance rate per group.\n",
    "    threshold: score threshold to 'accept' applicant.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for a in applicants:\n",
    "        g = a.get(group_key, 'unknown')\n",
    "        groups[g].append(a)\n",
    "\n",
    "    results = {}\n",
    "    for g, items in groups.items():\n",
    "        scores = [score_fn(x) for x in items]\n",
    "        avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "        accept_rate = sum(1 for s in scores if s >= threshold) / len(scores) if scores else 0.0\n",
    "        results[g] = {'avg_score': avg_score, 'accept_rate': accept_rate, 'n': len(scores)}\n",
    "    return results\n",
    "\n",
    "def counterfactual_invariance_check(applicants: List[Dict], score_fn: Callable, sensitive_attr: str = 'gender', allowed_diff: float = 1e-6):\n",
    "    \"\"\"\n",
    "    For each applicant, flip the sensitive attribute and recompute score.\n",
    "    Count how many change beyond allowed_diff.\n",
    "    Returns fraction changed.\n",
    "    Note: This test is for deterministic score functions that read the sensitive attribute.\n",
    "    \"\"\"\n",
    "    changed = 0\n",
    "    total = 0\n",
    "    for a in applicants:\n",
    "        total += 1\n",
    "        a_copy = deepcopy(a)\n",
    "        # flip gender placeholder (male<->female), leave 'other' unchanged for simplicity\n",
    "        if a_copy.get(sensitive_attr) == 'male':\n",
    "            a_copy[sensitive_attr] = 'female'\n",
    "        elif a_copy.get(sensitive_attr) == 'female':\n",
    "            a_copy[sensitive_attr] = 'male'\n",
    "        else:\n",
    "            # try swapping to male\n",
    "            a_copy[sensitive_attr] = 'male'\n",
    "        s_orig = score_fn(a)\n",
    "        s_cf = score_fn(a_copy)\n",
    "        if abs(s_orig - s_cf) > allowed_diff:\n",
    "            changed += 1\n",
    "    return {'changed': changed, 'total': total, 'fraction_changed': changed/total if total else 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e15d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive scorer stats:\n",
      "{'male': {'avg_score': 11.704725806451613, 'accept_rate': 0.0, 'n': 496}, 'female': {'avg_score': 10.814525793650793, 'accept_rate': 0.0, 'n': 504}}\n",
      "Naive counterfactual changes: {'changed': 1000, 'total': 1000, 'fraction_changed': 1.0}\n",
      "\n",
      "Safe scorer stats:\n",
      "{'male': {'avg_score': 51.92, 'accept_rate': 0.4032258064516129, 'n': 496}, 'female': {'avg_score': 50.40561507936508, 'accept_rate': 0.31547619047619047, 'n': 504}}\n",
      "Safe counterfactual changes: {'changed': 0, 'total': 1000, 'fraction_changed': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# simulate_bias.py\n",
    "import random\n",
    "# Note: group_stats and counterfactual_invariance_check are defined in another notebook cell (bias_checks.py content).\n",
    "# Similarly, score_applicant_naive and score_applicant_safe are defined in earlier cells.\n",
    "# In a Jupyter notebook you can use those functions directly without importing a module.\n",
    "\n",
    "def make_synthetic(n=500, bias_in_labels=False):\n",
    "    names = ['A','B','C','D']\n",
    "    genders = ['male', 'female']\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        gender = random.choice(genders)\n",
    "        edu = random.randint(10, 20)\n",
    "        exp = random.randint(0, 20)\n",
    "        skill = random.uniform(30, 95)\n",
    "        certs = random.randint(0, 5)\n",
    "        # optionally bias labels: e.g., lower skill for female (simulating historical bias)\n",
    "        if bias_in_labels and gender == 'female':\n",
    "            skill -= 5.0  # simulating biased historical outcomes\n",
    "        data.append({'education_years': edu, 'years_experience': exp, 'skill_score': skill, 'certifications_count': certs, 'gender': gender})\n",
    "    return data\n",
    "\n",
    "# create data\n",
    "applicants = make_synthetic(1000, bias_in_labels=True)\n",
    "\n",
    "# compute stats\n",
    "print(\"Naive scorer stats:\")\n",
    "print(group_stats(applicants, score_applicant_naive, threshold=55.0, group_key='gender'))\n",
    "print(\"Naive counterfactual changes:\", counterfactual_invariance_check(applicants, score_applicant_naive))\n",
    "\n",
    "print(\"\\nSafe scorer stats:\")\n",
    "print(group_stats(applicants, score_applicant_safe, threshold=55.0, group_key='gender'))\n",
    "print(\"Safe counterfactual changes:\", counterfactual_invariance_check(applicants, score_applicant_safe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4302893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_counterfactual_safe():\n",
    "    base = {'education_years': 16, 'years_experience': 5, 'skill_score': 80.0, 'certifications_count': 2, 'gender': 'female'}\n",
    "    copy = base.copy()\n",
    "    copy['gender'] = 'male'\n",
    "    assert score_applicant_safe(base) == score_applicant_safe(copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
